RGB-D -> VLM -> pyoctomap Pipeline Roadmap

Assumptions
- Offline testing will start with folder-mode RGB-D data (easier than live drone hardware).
- Depth images are either 16-bit PNG in millimeters or float images in meters.(making sure friday)
- pyoctomap is going to be added later; if unavailable the pipeline will fall back to Open3D PLY output.

High-level contract
- Inputs: RGB (H×W×3) images and depth (H×W) aligned to RGB; optional per-frame camera pose.
- Outputs: per-frame VLM scores/labels; accumulated 3D map (OctoMap if pyoctomap installed, else PLY point cloud); visualizations and per-frame JSON metadata.
- Success: pipeline can process a folder of RGB-D frames, produce a readable map, and produce per-frame VLM labels with reasonable initial accuracy on a sample dataset.

Step-by-step roadmap

1) Define requirements and success criteria
   - Decide offline vs real-time, camera model (intrinsics), desired map resolution/voxel size, and evaluation metrics (precision/recall for glass, map quality metrics).
   - Deliverable: a short spec file (YAML) listing inputs, outputs, and metrics.

2) Set up environment and dependencies
   - Create virtual environment and install pinned packages (torch, transformers, open3d, pillow, numpy, tqdm, pyrealsense2, pyoctomap).
   - Verify GPU availability if using CUDA.
   - Deliverable: working environment and notes in README.

3) Collect sample RGB-D data
   - Prepare a small dataset (20–200 frames) with glass/transparent and non-glass scenes.
   - Place matching RGB/depth files into data/rgb and data/depth for folder-mode testing.
   - Label a small subset with ground truth for evaluation.

4) Finalize frame source and metadata
   - Ensure the frame source yields (rgb_pil, depth_m, meta) where meta contains timestamp, optional intrinsics, and placeholders for pose.
   - Deliverable: tested `camera.py` and a short script to load a single pair.

5) Integrate VLM (CLIP)
   - Implement `vlm.py` to compute normalized image and text embeddings, support batching, and compute cosine similarities.
   - Create an example that scores each frame against glass-related queries ("glass window", "transparent surface", "mirror").
   - Deliverable: top-label per frame and scores CSV.

6) (Optional) Add region proposals / object detection
   - Integrate an object detector (NanoOwl, DINOV2) to crop regions for region-level CLIP scoring, improving object discrimination.
   - Keep detector optional so frame-level scoring still works.

7) Convert RGB-D to point cloud and local mapping
   - Use Open3D to convert each RGB-D frame into a point cloud.
   - If camera poses are available, apply them before merging; otherwise perform local alignment (ICP) or simple merging.
   - Deliverable: per-frame PLYs and combined PLY.

8) Integrate pyoctomap
   - If `pyoctomap` is available, create an octree with chosen resolution and insert scans using sensor origin and raycasts for occupancy updates.
   - If not available, continue with voxel downsampled Open3D PLY as fallback.

9) Pose handling & loop closure (optional)
   - Integrate drone odometry/IMU or a SLAM backend if global consistency is required. Otherwise allow offline reprocessing with improved poses.

10) Visualization & debugging tools
   - Annotate images with VLM labels and scores, provide Open3D viewer scripts, and save per-frame JSON metadata (label, score, insertion status).

11) Evaluation & tests
   - Run pipeline on labeled dataset; compute detection metrics and basic map quality checks.
   - Add smoke tests and unit tests for core modules.

12) Performance tuning & deployment
   - Profile VLM inference and mapping steps. Add batching, lower-res inference, or quantized models for edge deployment if needed.

13) Optional: produce occupancy / traversability map
   - From OctoMap occupancy, slice to 2.5D occupancy grid and compute traversability for navigation.

Minimal Viable Pipeline (MVP)
- Offline folder-mode only
- Frame-level CLIP scoring (no detector)
- Convert frames to Open3D point clouds, merge simply (no global SLAM)
- Save combined PLY and per-frame CSV with best CLIP label and score

Risks and edge cases
- Transparent glass may produce noisy or missing depth; mitigate by fusing multiple viewpoints and conservative occupancy updates.
- pyoctomap can be tricky to install on Windows; consider Docker or using Open3D fallback.
- CLIP may be ambiguous; region proposals and prompt engineering help.

Next actions
- (A) Implement a small test harness that runs one RGB-D pair through CLIP and Open3D and saves outputs.
- (B) Add object detection integration for region-level CLIP scoring.
- (C) Help with pyoctomap installation and testing on your machine.
- (D) Produce the YAML spec file listing requirements and success criteria.
